{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moadtahri/gpt3/blob/master/Chatbot_with_custom_knowledge_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaLK4kD1Uar"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "In this notebook we are going to create custom knowledge base to analyse a transcript using GPT-3."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XiUyHP4T2g5F"
      },
      "source": [
        "#Install main dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6LL4rxT6_W7h"
      },
      "outputs": [],
      "source": [
        "#%pip install llama-index\n",
        "#%pip install langchain\n",
        "#%pip install openai"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FbuYetOy25eM"
      },
      "source": [
        "# Some helper functions\n",
        "The following code defines the functions we need to construct indexation of chunks of our transcript in order to overcome the limitations offered by OpenAI\n",
        "\n",
        "This section, the parameters are still subjects to fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UelAqQgk_yIt"
      },
      "outputs": [],
      "source": [
        "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def construct_index(directory_path):\n",
        "    # set maximum input size\n",
        "    max_input_size = 4096\n",
        "    # set number of output tokens\n",
        "    num_outputs = 2000\n",
        "    # set maximum chunk overlap\n",
        "    max_chunk_overlap = 20\n",
        "    # set chunk size limit\n",
        "    chunk_size_limit = 600 \n",
        "\n",
        "    # define prompt helper\n",
        "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        "\n",
        "    # define LLM model_name=\"text-davinci-003\"\n",
        "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"text-babbage-001\", max_tokens=num_outputs))\n",
        " \n",
        "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "    \n",
        "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    index.save_to_disk('index.json')\n",
        "\n",
        "    return index\n",
        "\n",
        "def consult_canyas():\n",
        "        \n",
        "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "    res=True\n",
        "    while res:\n",
        "            try:\n",
        "                query = input(\"Please enter you request:\")\n",
        "                response = index.query(query)\n",
        "                print(response)\n",
        "                display(Markdown(f\"Response: <b>{response.response}</b>\"))\n",
        "            except:\n",
        "                 print('empty request')\n",
        "                 res=False\n",
        "                 \n",
        "\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz1jp33jGumu"
      },
      "source": [
        "# OpenAI API Key\n",
        "You need an Openai API key to be able to run this code.\n",
        "\n",
        "If you don't have one yet, get it by [signing up](https://platform.openai.com/overview). Then click your account icon on the top right of the screen and select \"View API Keys\". Create an API key.\n",
        "\n",
        "Then run the code below and paste your API key into the text input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RoJHE4fsAT3w"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = input(\"Paste your OpenAI key here and hit enter:\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVrddlAL4I_v"
      },
      "source": [
        "# Indexation\n",
        "\n",
        "Now we are ready to construct the index. This will take every file in the folder 'data', split it into chunks, and embed it with OpenAI's embeddings API.\n",
        "\n",
        "**Notice:** running this code will cost you credits on your OpenAPI account ($0.02 for every 1,000 tokens). If you've just set up your account, the free credits that you have should be more than enough for this experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kCYTE2EqBB7O"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 13944 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex at 0x2517f7af4c0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "construct_index(\"knowledge_base/data\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ipJ_gYxN5cWh"
      },
      "source": [
        "# run some thematic coding and text mining analysis\n",
        "\n",
        "Here are a few queries you can run:\n",
        "1. Who are the individuals participating in this interview?\n",
        "2. What the main topics discussed?\n",
        "3. what are the main themes?\n",
        "4. what are the main relevant words with their counts?\n",
        "5. what are the pains of Rens Erwin van in bullet points?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s_uwsPGEIGsb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3053 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 2 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "about PKI spotlight?\n",
            "\n",
            "Muneer Mubashir and Rens, Erwin van talk about PKI spotlight.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Response: <b>about PKI spotlight?\n",
              "\n",
              "Muneer Mubashir and Rens, Erwin van talk about PKI spotlight.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
            "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
            "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
            "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
            "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
            "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "empty request\n"
          ]
        }
      ],
      "source": [
        "consult_canyas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "98e2bfb919ff5917b98b90a86d4547b4b7593f6b54c8d2e707cb1748df7f4086"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
