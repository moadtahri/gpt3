{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moadtahri/gpt3/blob/master/Chatbot_with_custom_knowledge_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaLK4kD1Uar"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "In this notebook we are going to create custom knowledge base to analyse a transcript using GPT-3."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XiUyHP4T2g5F"
      },
      "source": [
        "#Install main dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6LL4rxT6_W7h"
      },
      "outputs": [],
      "source": [
        "#%pip install llama-index\n",
        "#%pip install langchain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FbuYetOy25eM"
      },
      "source": [
        "# Some helper functions\n",
        "The following code defines the functions we need to construct indexation of chunks of our transcript in order to overcome the limitations offered by OpenAI\n",
        "\n",
        "This section, the parameters are still subjects to fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UelAqQgk_yIt"
      },
      "outputs": [],
      "source": [
        "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def construct_index(directory_path):\n",
        "    # set maximum input size\n",
        "    max_input_size = 4096\n",
        "    # set number of output tokens\n",
        "    num_outputs = 2000\n",
        "    # set maximum chunk overlap\n",
        "    max_chunk_overlap = 20\n",
        "    # set chunk size limit\n",
        "    chunk_size_limit = 600 \n",
        "\n",
        "    # define prompt helper\n",
        "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        "\n",
        "    # define LLM\n",
        "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
        " \n",
        "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "    \n",
        "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    index.save_to_disk('index.json')\n",
        "\n",
        "    return index\n",
        "\n",
        "def consult_canyas():\n",
        "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "    while True: \n",
        "        query = input(\"What do you want to ask? \")\n",
        "        response = index.query(query)\n",
        "        display(Markdown(f\"Response: <b>{response.response}</b>\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz1jp33jGumu"
      },
      "source": [
        "# OpenAI API Key\n",
        "You need an Openai API key to be able to run this code.\n",
        "\n",
        "If you don't have one yet, get it by [signing up](https://platform.openai.com/overview). Then click your account icon on the top right of the screen and select \"View API Keys\". Create an API key.\n",
        "\n",
        "Then run the code below and paste your API key into the text input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RoJHE4fsAT3w"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = input(\"Paste your OpenAI key here and hit enter:\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVrddlAL4I_v"
      },
      "source": [
        "# Indexation\n",
        "\n",
        "Now we are ready to construct the index. This will take every file in the folder 'data', split it into chunks, and embed it with OpenAI's embeddings API.\n",
        "\n",
        "**Notice:** running this code will cost you credits on your OpenAPI account ($0.02 for every 1,000 tokens). If you've just set up your account, the free credits that you have should be more than enough for this experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kCYTE2EqBB7O"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 13944 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex at 0x1c2ddcf1400>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "construct_index(\"knowledge_base/data\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ipJ_gYxN5cWh"
      },
      "source": [
        "# run some thematic coding and text mining analysis\n",
        "\n",
        "Here are a few queries you can run:\n",
        "1. Who are the individuals participating in this interview?\n",
        "2. What the main topics discussed?\n",
        "3. what are the main themes?\n",
        "4. what are the main relevant words with their counts?\n",
        "5. what are the pains of Rens Erwin van in bullet points?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s_uwsPGEIGsb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3055 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 9 tokens\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Response: <b>\n",
              "The individuals participating in this interview are Muneer Mubashir and Rens Erwin van.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3076 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 6 tokens\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Response: <b>\n",
              "The main topics discussed are PKI Spotlight, best practices for PKI, NTLM authentication, threat analysis engines, template misconfiguration checks, and the need for organizations to be serious about their PKI environment.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4069 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 6 tokens\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Response: <b>\n",
              "\n",
              "The main themes discussed in the context information are cyber security, compliance, PKI, threat landscape, attack surface, risk assessment, encryption, smart card logons, BitLocker encryption, and simplifying the use of PKI certificates for end user logons.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4278 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 10 tokens\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Response: <b>\n",
              "PKI - 5\n",
              "Certificates - 5\n",
              "Security - 5\n",
              "Data - 5\n",
              "Risk - 5\n",
              "Protection - 4\n",
              "Attack - 3\n",
              "Recovery - 3\n",
              "Active Directory - 3\n",
              "Smart Card - 3\n",
              "Logons - 3\n",
              "Encryption - 3\n",
              "Internet - 2\n",
              "Vendors - 2\n",
              "Vouchers - 2\n",
              "Threat - 2\n",
              "Landscape - 2\n",
              "Business - 2\n",
              "Access - 2\n",
              "Systems - 2\n",
              "BitLocker - 1\n",
              "Recovery Keys - 1\n",
              "Smart Card Logons - 1\n",
              "PKI Generated Certificates - 1\n",
              "Simplifying - 1</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4216 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Response: <b>\n",
              "\n",
              "The pains of Rens Erwin van are that organizations do not understand the value of a PKI and the importance of protecting the keys. He also sees that organizations are choosing the cheapest certificate vendors instead of the most trustworthy, and that the Dutch government stopped their PKI hierarchy. He also believes that organizations lack the awareness of misconfigurations and vulnerabilities in their DCS that can expose the entire organization, as well as the lack of an internal certificate policy, trust policy, or guidelines on how to request and obtain certificates, and what can and cannot be used in a certificate.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 4252 tokens\n",
            "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 14 tokens\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Response: <b>\n",
              "\n",
              "• Difficulty in demonstrating the value of PKI to organizations\n",
              "• Organizations opting for cheaper certificates instead of trusted certificates\n",
              "• Lack of government decree requiring government organizations to use trusted certificates\n",
              "• Difficulty in showing the difference in value between trusted and untrusted certificates\n",
              "• Expensive HSM solutions\n",
              "• Lack of visibility and understanding of misconfigurations and vulnerabilities in PKI systems\n",
              "• Lack of organizational maturity and awareness of misconfigurations and vulnerabilities in PKI systems\n",
              "• Difficulty in finding organizations with an internal certificate policy or trust policy\n",
              "• Difficulty in obtaining certificates with proper guidelines on how to request and use them</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8740\\3342493043.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconsult_canyas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8740\\489338007.py\u001b[0m in \u001b[0;36mconsult_canyas\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPTSimpleVectorIndex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"What do you want to ask? \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Response: <b>{response.response}</b>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             )\n\u001b[1;32m-> 1177\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1217\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1219\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1220\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "consult_canyas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "98e2bfb919ff5917b98b90a86d4547b4b7593f6b54c8d2e707cb1748df7f4086"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
